{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dress-voltage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "based-tonight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "imdb_data=pd.read_csv('data/IMDB_reviews.csv')\n",
    "\n",
    "# print(imdb_data.shape,'\\n')\n",
    "# print(imdb_data.head(10),'\\n')\n",
    "# print(imdb_data['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-sitting",
   "metadata": {},
   "source": [
    "### Step 1 - Preprocessing Text to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "numerical-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fitting-amendment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 11 Âµs\n"
     ]
    }
   ],
   "source": [
    "# %time\n",
    "# # Pre-processing text data\n",
    "\n",
    "# def preprocessing(text):\n",
    "#     soup = BeautifulSoup(text, \"html.parser\")\n",
    "#     text = soup.get_text()     # Step 1 - Remove html tags\n",
    "#     pattern = r'[^a-zA-z0-9\\s]' \n",
    "#     text = re.sub(pattern,'',text)   # Step 2 - Remove punctuation\n",
    "#     text = text.lower()     # Step 3 - Make text lowercase\n",
    "#     tokenizer=ToktokTokenizer()\n",
    "#     tokens = tokenizer.tokenize(text)    # Step 4 - Tokenizing\n",
    "#     lemma = WordNetLemmatizer()          # Step 5 - Lemmatizing\n",
    "#     stopword_list=nltk.corpus.stopwords.words('english')\n",
    "#     stop=set(stopwords.words('english'))\n",
    "#     filtered_tokens = [lemma.lemmatize(word).strip() for word in tokens if word not in stopword_list]    # Step 6 - Removing stopwords\n",
    "#     text = TreebankWordDetokenizer().detokenize(filtered_tokens)\n",
    "#     return text\n",
    "\n",
    "# #Apply function on review column\n",
    "# imdb_data['review'] = imdb_data['review'].apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "superb-sullivan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imdb_data.to_csv('data/IMDB_reviews_preprocessed.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceramic-handbook",
   "metadata": {},
   "source": [
    "### Step 2 - Tokens to features and train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adaptive-master",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one reviewer mentioned watching 1 oz episode y...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wonderful little production filming technique ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there family little boy jake think t...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter matteis love time money visually stunni...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one reviewer mentioned watching 1 oz episode y...  positive\n",
       "1  wonderful little production filming technique ...  positive\n",
       "2  thought wonderful way spend time hot summer we...  positive\n",
       "3  basically there family little boy jake think t...  negative\n",
       "4  petter matteis love time money visually stunni...  positive"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data = pd.read_csv('data/IMDB_reviews_preprocessed.csv')\n",
    "imdb_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "political-income",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_train_reviews=imdb_data.review[:40000]\n",
    "norm_test_reviews=imdb_data.review[40000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "headed-hello",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "# Count vectorizer for bag of words\n",
    "cv = CountVectorizer(\n",
    "        tokenizer=dummy,\n",
    "        preprocessor=dummy,\n",
    "#        ngram_range=(1,3)\n",
    "    )  \n",
    "\n",
    "#transformed train reviews\n",
    "cv_train_reviews=cv.fit_transform(norm_train_reviews)\n",
    "cv_test_reviews=cv.transform(norm_test_reviews)\n",
    "\n",
    "# Tf-idf vectorizer for bag of words\n",
    "tfidf = TfidfVectorizer(\n",
    "        tokenizer=dummy,\n",
    "        preprocessor=dummy\n",
    "    )  \n",
    "tfidf_train_reviews = tfidf.fit_transform(norm_train_reviews)\n",
    "tfidf_test_reviews = tfidf.transform(norm_test_reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "accompanied-invention",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 1)\n"
     ]
    }
   ],
   "source": [
    "#labeling the sentiment data\n",
    "lb=LabelBinarizer()\n",
    "\n",
    "#transformed sentiment data\n",
    "sentiment_data=lb.fit_transform(imdb_data['sentiment'])\n",
    "print(sentiment_data.shape)\n",
    "\n",
    "#Spliting the sentiment data\n",
    "train_sentiments=sentiment_data[:40000]\n",
    "test_sentiments=sentiment_data[40000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-string",
   "metadata": {},
   "source": [
    "### Step 3 - Fit and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "official-galaxy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1, max_iter=500, random_state=42)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training the model\n",
    "lr=LogisticRegression(penalty='l2',max_iter=500,C=1,random_state=42)\n",
    "\n",
    "#Fitting the model for Bag of words\n",
    "lr_bow=lr.fit(cv_train_reviews,train_sentiments)\n",
    "print(lr_bow)\n",
    "\n",
    "#Predicting the model for bag of words\n",
    "lr_bow_predict=lr.predict(cv_test_reviews)\n",
    "lr_bow_predict\n",
    "# lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dimensional-japanese",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1, max_iter=500, random_state=42)\n"
     ]
    }
   ],
   "source": [
    "#Fitting the model for tfidf features\n",
    "lr_tfidf=lr.fit(tfidf_train_reviews,train_sentiments)\n",
    "print(lr_tfidf)\n",
    "\n",
    "#Predicting the model for tfidf features\n",
    "lr_tfidf_predict=lr.predict(tfidf_test_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-plate",
   "metadata": {},
   "source": [
    "### Step 4 - Metrics Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "chubby-mistake",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_bow_accuracy : 0.6173\n",
      "\n",
      "lr_bow_classification_report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.62      0.62      0.62      4993\n",
      "    Negative       0.62      0.62      0.62      5007\n",
      "\n",
      "    accuracy                           0.62     10000\n",
      "   macro avg       0.62      0.62      0.62     10000\n",
      "weighted avg       0.62      0.62      0.62     10000\n",
      "\n",
      "lr_bow_confusion_matrix:\n",
      " [[3100 1907]\n",
      " [1920 3073]]\n"
     ]
    }
   ],
   "source": [
    "#Accuracy score for bag of words\n",
    "lr_bow_score=accuracy_score(test_sentiments,lr_bow_predict)\n",
    "print(\"lr_bow_accuracy :\",lr_bow_score)\n",
    "\n",
    "#Classification report for bag of words \n",
    "lr_bow_report=classification_report(test_sentiments,lr_bow_predict,target_names=['Positive','Negative'])\n",
    "print('\\nlr_bow_classification_report:\\n', lr_bow_report)\n",
    "\n",
    "#confusion matrix for bag of words\n",
    "cm_bow=confusion_matrix(test_sentiments,lr_bow_predict,labels=[1,0])\n",
    "print('lr_bow_confusion_matrix:\\n', cm_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "medical-wheat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_tfidf_accuracy : 0.616\n",
      "\n",
      "lr_tfidf_classification_report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.62      0.61      0.61      4993\n",
      "    Negative       0.61      0.63      0.62      5007\n",
      "\n",
      "    accuracy                           0.62     10000\n",
      "   macro avg       0.62      0.62      0.62     10000\n",
      "weighted avg       0.62      0.62      0.62     10000\n",
      "\n",
      "lr_tfidf_confusion_matrix:\n",
      " [[3138 1869]\n",
      " [1971 3022]]\n"
     ]
    }
   ],
   "source": [
    "#Accuracy score for tfidf features\n",
    "lr_tfidf_score=accuracy_score(test_sentiments,lr_tfidf_predict)\n",
    "print(\"lr_tfidf_accuracy :\",lr_tfidf_score)\n",
    "\n",
    "#Classification report for tfidf features\n",
    "lr_tfidf_report=classification_report(test_sentiments,lr_tfidf_predict,target_names=['Positive','Negative'])\n",
    "print('\\nlr_tfidf_classification_report:\\n', lr_tfidf_report)\n",
    "\n",
    "#confusion matrix for tfidf features\n",
    "cm_tfidf=confusion_matrix(test_sentiments,lr_tfidf_predict,labels=[1,0])\n",
    "print('lr_tfidf_confusion_matrix:\\n', cm_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "authorized-integrity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB()\n",
      "[1 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "mnb=MultinomialNB()\n",
    "\n",
    "#fitting the svm for bag of words\n",
    "mnb_bow=mnb.fit(cv_train_reviews,train_sentiments)\n",
    "print(mnb_bow)\n",
    "\n",
    "#Predicting the model for bag of words\n",
    "mnb_bow_predict=mnb.predict(cv_test_reviews)\n",
    "print(mnb_bow_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "statistical-statement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB()\n",
      "[0 0 1 ... 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "#fitting the svm for tfidf features\n",
    "mnb_tfidf=mnb.fit(tfidf_train_reviews,train_sentiments)\n",
    "print(mnb_tfidf)\n",
    "\n",
    "#Predicting the model for tfidf features\n",
    "mnb_tfidf_predict=mnb.predict(tfidf_test_reviews)\n",
    "print(mnb_tfidf_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "figured-presence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnb_bow_score : 0.6084\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.60      0.63      0.62      4993\n",
      "    Negative       0.61      0.59      0.60      5007\n",
      "\n",
      "    accuracy                           0.61     10000\n",
      "   macro avg       0.61      0.61      0.61     10000\n",
      "weighted avg       0.61      0.61      0.61     10000\n",
      "\n",
      "[[2945 2062]\n",
      " [1854 3139]]\n"
     ]
    }
   ],
   "source": [
    "#Accuracy score for bag of words\n",
    "mnb_bow_score=accuracy_score(test_sentiments,mnb_bow_predict)\n",
    "print(\"mnb_bow_score :\",mnb_bow_score)\n",
    "\n",
    "#Classification report for bag of words \n",
    "mnb_bow_report=classification_report(test_sentiments,mnb_bow_predict,target_names=['Positive','Negative'])\n",
    "print(mnb_bow_report)\n",
    "\n",
    "#confusion matrix for bag of words\n",
    "cm_bow=confusion_matrix(test_sentiments,mnb_bow_predict,labels=[1,0])\n",
    "print(cm_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "surprised-friend",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnb_tfidf_score : 0.6135\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.61      0.63      0.62      4993\n",
      "    Negative       0.62      0.60      0.61      5007\n",
      "\n",
      "    accuracy                           0.61     10000\n",
      "   macro avg       0.61      0.61      0.61     10000\n",
      "weighted avg       0.61      0.61      0.61     10000\n",
      "\n",
      "[[3005 2002]\n",
      " [1863 3130]]\n"
     ]
    }
   ],
   "source": [
    "#Accuracy score for tfidf features\n",
    "mnb_tfidf_score=accuracy_score(test_sentiments,mnb_tfidf_predict)\n",
    "print(\"mnb_tfidf_score :\",mnb_tfidf_score)\n",
    "\n",
    "#Classification report for tfidf features\n",
    "mnb_tfidf_report=classification_report(test_sentiments,mnb_tfidf_predict,target_names=['Positive','Negative'])\n",
    "print(mnb_tfidf_report)\n",
    "\n",
    "#confusion matrix for tfidf features\n",
    "cm_tfidf=confusion_matrix(test_sentiments,mnb_tfidf_predict,labels=[1,0])\n",
    "print(cm_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understood-cardiff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-fitting",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
